{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FS4oxM9gDMYu"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_map(X, Y, model):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        X: tensor of images with shape (N, 3, H, W) where 1. N is the number of images, 2. 3 is the number of color channels (RGB), 3. H and W are the height/width of each image.\n",
        "        Y: labels for X: contains shape (N, ) which contains class labels for each image in X\n",
        "        model: the CNN model used to calculate saliency maps\n",
        "\n",
        "    Returns:\n",
        "        saliency: a tensor of shape (N, H, W) giving saliency maps for the input images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X_var = Variable(X, requires_grad=True) # allows us to compute gradients\n",
        "    Y_var = Variable(y)\n",
        "\n",
        "    scores = model.forward(X_var) # calculates model's output scores for each image\n",
        "    loss = torch.sum(scores.gather(1, Y_var.view(-1, 1)).squeeze())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    grad = torch.abs(X_var.grad.data)\n",
        "    saliency = torch.max(grad, 1)[0].squeeze()\n",
        "\n",
        "    return saliency"
      ],
      "metadata": {
        "id": "hzxfIE6PDO-8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(X, Y):\n",
        "\n",
        "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0) # Converts list of images into a list of processed PIL images\n",
        "    Y_tensor = torch.LongTensor(y)\n",
        "\n",
        "    saliency = compute_map(X_tensor, Y_tensor, model)\n",
        "    saliency = saliency.numpy()\n",
        "\n",
        "    N = X.shape[0]\n",
        "    for i in range(N):\n",
        "        plt.subplot(2, N, i + 1)\n",
        "        plt.imshow(X[i])\n",
        "        plt.axis('off')\n",
        "        plt.subplot(2, N, N + i + 1)\n",
        "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
        "        plt.axis('off')\n",
        "        plt.gcf().set_size_inches(12, 5)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UgOaO8ynDRLO"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}